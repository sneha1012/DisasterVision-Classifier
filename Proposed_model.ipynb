{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKe9zow2P2/daG6v6pG55B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneha1012/DisasterVision-Classifier/blob/main/Proposed_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUNDVHELGz8s"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries for the U-Net Model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#U-Net Model\n",
        "def multi_unet_model(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
        "\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "    s = inputs\n",
        "\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "    #Above are Convolutional layers with relu activation and Softmax at the end of the model for results\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs]) #The model is saved in a variable 'model'\n",
        "\n",
        "\n",
        "    return model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries for running the model\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import sklearn.utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "numbers = re.compile(r'(\\d+)')\n",
        "\n",
        "#Numerical Sorting for taking data in sequential order\n",
        "def numericalSort(value):\n",
        "    parts = numbers.split(value)\n",
        "    parts[1::2] = map(int, parts[1::2])\n",
        "    return parts\n",
        "\n",
        "#Function defines for focal loss\n",
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\tdef focal_loss_fixed(y_true, y_pred):\n",
        "\t\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "\t\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\t\treturn -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1+K.epsilon())) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
        "\treturn focal_loss_fixed\n",
        "\n",
        "#Changing size of the dataset images\n",
        "SIZE_X = 256\n",
        "SIZE_Y = 256\n",
        "\n",
        "#Number of Classes\n",
        "n_classes=6\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Storing the dataset images\n",
        "train_images = []\n",
        "for directory_path in glob.glob(\"../Dataset_Images_Path\"):\n",
        "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.png\")), key = numericalSort):\n",
        "        img = cv2.imread(img_path, 0)\n",
        "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        train_images.append(img)\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "\n",
        "#Storing the masks of those images\n",
        "train_masks = []\n",
        "for directory_path in glob.glob(\"../Mask_Created_Path\"):\n",
        "    for mask_path in sorted(glob.glob(os.path.join(directory_path, \"*.png\")), key = numericalSort):\n",
        "        mask = cv2.imread(mask_path, 0)\n",
        "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)\n",
        "        train_masks.append(mask)\n",
        "\n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Giving labels to each class present in the masks\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "np.unique(train_masks_encoded_original_shape)\n",
        "\n",
        "#Normalizing the dataset\n",
        "train_images = np.expand_dims(train_images, axis=3)\n",
        "train_images = tf.keras.utils.normalize(train_images, axis = 1)\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "\n",
        "\n",
        "#Splitting the Dataset into testing and validation data\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
        "train_masks_cat = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "test_masks_cat = tf.keras.utils.to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
        "\n",
        "\n",
        "#Assigning weights to each of the clss according to their ocurence in the dataset\n",
        "sample_weight = np.zeros((105, 65536))\n",
        "sample_weight[:, 0] += 414.34 # un-classified\n",
        "sample_weight[:, 1] += 0.02 # No-Label in Json\n",
        "sample_weight[:, 2] += 0.03 # no-damage\n",
        "sample_weight[:, 3] += 1726.4 # minor-damage\n",
        "sample_weight[:, 4] += 298.8 #major-damage\n",
        "sample_weight[:, 5] += 15537.5 #destroyed\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "IMG_HEIGHT = X_train.shape[1]\n",
        "IMG_WIDTH  = X_train.shape[2]\n",
        "IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Calling the defined model\n",
        "model = multi_unet_model(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "model.compile(optimizer='adam', loss=[focal_loss(alpha=.25, gamma=2)], metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#Running the model\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    batch_size = 1,\n",
        "                    verbose=1,\n",
        "                    epochs=125,\n",
        "                    validation_data=(X_test, y_test_cat),\n",
        "                    sample_weight = sample_weight,\n",
        "                    shuffle=False)\n",
        "\n",
        "\n",
        "#Saving the model\n",
        "model.save('test.hdf5')\n"
      ],
      "metadata": {
        "id": "iDVctBxkG3md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the graphs of Loss and Accuracy\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#Load the model to run on the Testing data\n",
        "model.load_weights('test.hdf5')\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "from keras.metrics import MeanIoU\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)\n",
        "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
        "_, acc = model.evaluate(X_test, y_test_cat)\n",
        "recall = recall_score(y_test.flatten(), y_pred_argmax.flatten(),average='weighted')\n",
        "precision = precision_score(y_test.flatten(),y_pred_argmax.flatten(),average='weighted')\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy()) #Printing IoU\n",
        "print(\"Accuracy is = \", (acc * 100.0), \"%\")     #Printing Accuracy\n",
        "print('Precision: ',precision)                  #Printing Precision\n",
        "print('Recall: ',recall)                        #Printing Recell\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Loading Testing data\n",
        "import random\n",
        "for i in range(12):\n",
        "  test_img = X_test[i]\n",
        "  ground_truth=y_test[i]\n",
        "  test_img_norm=test_img[:,:,0][:,:,None]\n",
        "  test_img_input=np.expand_dims(test_img_norm, 0)\n",
        "  prediction = (model.predict(test_img_input))\n",
        "  predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "\n",
        "#Showing the Result\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plt.subplot(231)\n",
        "  plt.title('Testing Image')\n",
        "  plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "  plt.subplot(232)\n",
        "  plt.title('Testing Label')\n",
        "  plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
        "  plt.subplot(233)\n",
        "  plt.title('Prediction on test image')\n",
        "  plt.imshow(predicted_img, cmap='jet')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AAmpKAtOG9UH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}